{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb8f6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== FINAL COMPARISON TABLE ===\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "| | **FERD Acc.** | **AffectNet Acc.** |\n",
       "|---|---|---|\n",
       "| n | 152 | 31002 |\n",
       "| Mapped / Total | 40 / 40 | 40 / 40 |\n",
       "| **Emotion** | | |\n",
       "| Anger | 0.7368 | 0.7705 |\n",
       "| Contempt | 0.3158 | 0.2875 |\n",
       "| Disgust | 0.7895 | 0.4053 |\n",
       "| Fear | 1.0000 | 0.6908 |\n",
       "| Happy | 1.0000 | 0.9925 |\n",
       "| Neutral | 0.8421 | 0.7896 |\n",
       "| Sad | 0.7895 | 0.8394 |\n",
       "| Surprise | 0.7895 | 0.9870 |\n",
       "| **Overall** | | |\n",
       "| Mean Acc. | **0.7829** | **0.7572** |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Evaluation script for EMONET-FACE benchmark mappings on FERD and AffectNet.\n",
    "---------------------------------------------------------------------------\n",
    "This script evaluates model predictions on FERD and AffectNet datasets\n",
    "using dataset-specific emotion mappings. It computes overall and per-emotion\n",
    "accuracies and outputs a Markdown comparison table.\n",
    "\n",
    "Author: LAION AI\n",
    "\n",
    "NOTE: Please use the merge.sh split before you continue\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from config import parameters, affectnet_mapping, ferd_mapping\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Utility function: compute emotion accuracies\n",
    "# ----------------------------------------------------------------------\n",
    "def evaluate_dataset(df, gt_col, results_col, emotion_mapping):\n",
    "    \"\"\"\n",
    "    Compute per-emotion and overall accuracy after mapping model predictions.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Dataset with ground truth and prediction results.\n",
    "        gt_col (str): Column with ground truth emotion labels.\n",
    "        results_col (str): Column containing prediction dictionaries.\n",
    "        emotion_mapping (dict): Mapping from fine-grained to target categories.\n",
    "\n",
    "    Returns:\n",
    "        dict: {overall_accuracy, emotion_accuracies, n, mapped_total_ratio}\n",
    "    \"\"\"\n",
    "    n_mapped = sum(v is not None for v in emotion_mapping.values())\n",
    "    total = len(emotion_mapping)\n",
    "    cutoff = 1.5 / n_mapped\n",
    "\n",
    "    def is_correct(row):\n",
    "        gt = row[gt_col]\n",
    "        preds = [\n",
    "            (emotion_mapping.get(k), v[\"mean_subtracted\"])\n",
    "            for k, v in row[results_col].items()\n",
    "            if emotion_mapping.get(k)\n",
    "        ]\n",
    "        if not preds or pd.isna(gt):\n",
    "            return False\n",
    "        emotions, vals = zip(*preds)\n",
    "        sm = np.exp(vals) / np.exp(vals).sum()\n",
    "        return any(e == gt and s > cutoff for e, s in zip(emotions, sm))\n",
    "\n",
    "    df[\"is_correct\"] = df.apply(is_correct, axis=1)\n",
    "    return {\n",
    "        \"overall_accuracy\": df[\"is_correct\"].mean(),\n",
    "        \"emotion_accuracies\": df.groupby(gt_col)[\"is_correct\"].mean().to_dict(),\n",
    "        \"n\": len(df),\n",
    "        \"mapped_total_ratio\": f\"{n_mapped} / {total}\",\n",
    "    }\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Load and evaluate all datasets\n",
    "# ----------------------------------------------------------------------\n",
    "def load_datasets(params):\n",
    "    \"\"\"Load and prepare datasets according to the provided configuration.\"\"\"\n",
    "    datasets = []\n",
    "    for item in params:\n",
    "        df = pd.read_json(item[\"dataset\"], lines=True)\n",
    "        emotion_col = item[\"emotion_col\"]\n",
    "        df[\"gt_emotion_final\"] = (\n",
    "            df[emotion_col].apply(\n",
    "                lambda x: x.split(\"_\")[-1].split(\".\")[0].lower().replace(\"surprised\", \"surprise\")\n",
    "            )\n",
    "            if \"ferd.jsonl\" in item[\"dataset\"]\n",
    "            else df[emotion_col].map(item.get(\"gt_mapping\"))\n",
    "        )\n",
    "        datasets.append(\n",
    "            (\n",
    "                \"FERD\" if \"ferd\" in item[\"dataset\"] else \"AffectNet\",\n",
    "                df,\n",
    "                \"gt_emotion_final\",\n",
    "                item[\"results_col\"],\n",
    "            )\n",
    "        )\n",
    "    return datasets\n",
    "\n",
    "datasets = load_datasets(parameters)\n",
    "\n",
    "final_results, all_emotions = {}, set()\n",
    "for name, df, gt_col, res_col in datasets:\n",
    "    mapping = ferd_mapping if name == \"FERD\" else affectnet_mapping\n",
    "    result = evaluate_dataset(df, gt_col, res_col, mapping)\n",
    "    final_results[name] = result\n",
    "    all_emotions.update(result[\"emotion_accuracies\"].keys())\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Generate Markdown comparison table\n",
    "# ----------------------------------------------------------------------\n",
    "emotions = sorted(all_emotions)\n",
    "ferd, aff = final_results.get(\"FERD\", {}), final_results.get(\"AffectNet\", {})\n",
    "\n",
    "def fmt(x): return f\"{x:.4f}\" if x is not None else \"-\"\n",
    "\n",
    "md = [\n",
    "    \"| | **FERD Acc.** | **AffectNet Acc.** |\",\n",
    "    \"|---|---|---|\",\n",
    "    f\"| n | {ferd.get('n', '-')} | {aff.get('n', '-')} |\",\n",
    "    f\"| Mapped / Total | {ferd.get('mapped_total_ratio', '-')} | {aff.get('mapped_total_ratio', '-')} |\",\n",
    "    \"| **Emotion** | | |\",\n",
    "]\n",
    "for emo in emotions:\n",
    "    md.append(f\"| {emo.capitalize()} | {fmt(ferd.get('emotion_accuracies', {}).get(emo))} | {fmt(aff.get('emotion_accuracies', {}).get(emo))} |\")\n",
    "\n",
    "md.append(\"| **Overall** | | |\")\n",
    "md.append(f\"| Mean Acc. | **{fmt(ferd.get('overall_accuracy'))}** | **{fmt(aff.get('overall_accuracy'))}** |\")\n",
    "\n",
    "md_table = \"\\n\".join(md)\n",
    "\n",
    "print(\"\\n=== FINAL COMPARISON TABLE ===\")\n",
    "display(Markdown(md_table))\n",
    "\n",
    "with open(\"results_ferd_affectnet.md\", \"w\") as f:\n",
    "    f.write(md_table)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emonet-face",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

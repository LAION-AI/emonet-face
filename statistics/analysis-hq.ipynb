{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab67792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# This script computes inter-annotator agreement and model-vs-human agreement\n",
    "# using Cohen's Weighted Kappa, Krippendorff's Alpha, and bootstrapped confidence\n",
    "# intervals (95% CI, computed via 1000 bootstrap samples, using np.random.default_rng(42)).\n",
    "# All statistical tests (Mann-Whitney U) and error bars are reported in the output tables\n",
    "# and figures. Error bars represent 95% bootstrap confidence intervals for mean differences.\n",
    "# All randomness is controlled by a fixed seed (42) for full reproducibility.\n",
    "# Please see the output directory for CSV, LaTeX, and figure files referenced in the logs.\n",
    "# ---------------------------------------------------------------------------------\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import ast\n",
    "import os\n",
    "\n",
    "\"\"\"\n",
    "Compute Krippendorff's alpha for inter-annotator agreement.\n",
    "\n",
    "Notes:\n",
    "    - All error bars are 95% bootstrap confidence intervals (1000 samples, seed=42).\n",
    "    - Mann-Whitney U tests are used for group comparisons.\n",
    "    - All randomness is controlled by np.random.seed(42) and np.random.default_rng(42).\n",
    "    - See output logs for references to figures/tables with statistical results.\n",
    "\"\"\"\n",
    "\n",
    "def compute_krippendorffs_alpha(df, emotion_list):\n",
    "    \"\"\"\n",
    "    Compute Krippendorff's alpha for each emotion.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import krippendorff\n",
    "    except ImportError:\n",
    "        print(\"krippendorff package not installed. Please install with 'pip install krippendorff'.\")\n",
    "        return {}\n",
    "\n",
    "    results = {}\n",
    "    for emotion in emotion_list:\n",
    "        # Build matrix: rows=images, columns=annotators, values=annotation for this emotion\n",
    "        pivot = df.pivot_table(\n",
    "            index=\"image\", columns=\"annotator\",\n",
    "            values=lambda row: row[\"annotations\"].get(emotion, 0)\n",
    "        )\n",
    "        # krippendorff.alpha expects a list of lists (annotators x items)\n",
    "        data = pivot.values.T\n",
    "        # Remove columns (images) with all NaN\n",
    "        data = [list(col[~pd.isnull(col)]) for col in data]\n",
    "        # Only keep annotators with at least 2 ratings\n",
    "        data = [d for d in data if len(d) > 1]\n",
    "        if len(data) < 2:\n",
    "            results[emotion] = None\n",
    "            continue\n",
    "        alpha = krippendorff.alpha(reliability_data=data, level_of_measurement='interval')\n",
    "        results[emotion] = alpha\n",
    "    return results\n",
    "\n",
    "# Set matplotlib default DPI for consistent figure size (optional)\n",
    "try:\n",
    "    import matplotlib\n",
    "    matplotlib.rcParams['figure.dpi'] = 100\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "# --- Data Loading and Preprocessing ---\n",
    "\n",
    "os.makedirs(\"output\", exist_ok=True)\n",
    "\n",
    "# Read human annotation data\n",
    "all_csv = Path(\"../data\") / \"hq.csv\"\n",
    "if not all_csv.exists():\n",
    "    raise FileNotFoundError(f\"File {all_csv} not found.\")\n",
    "\n",
    "df = pd.read_csv(all_csv)\n",
    "if \"annotations\" not in df.columns:\n",
    "    raise ValueError(\"No 'annotations' column found in CSV.\")\n",
    "\n",
    "# Convert stringified dicts to dicts if necessary\n",
    "if isinstance(df[\"annotations\"].iloc[0], str):\n",
    "    df[\"annotations\"] = df[\"annotations\"].apply(ast.literal_eval)\n",
    "\n",
    "# For Hume-Face*, multiply all emotions by 7 (to match scale)\n",
    "df['annotations'] = df.apply(\n",
    "    lambda x: {k: v * 7 for k, v in x['annotations'].items()} if \"Hume Face*\" in x['annotator'] else x['annotations'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Get emotion list from first row, remove extra dimensions\n",
    "emotion_list = list(df[\"annotations\"].iloc[0].keys())\n",
    "emotion_list = [e for e in emotion_list if not e.startswith(\"Extra Dimensions|\")]\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "# --- Restructure Data for Analysis ---\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "# Convert to long format: one row per (image, annotator, emotion)\n",
    "long_data = []\n",
    "for _, row in df.iterrows():\n",
    "    annotator = row['annotator']\n",
    "    image = row['image']\n",
    "    human = row['human'] if 'human' in row else True\n",
    "    ratings = row['annotations']\n",
    "    for emotion in emotion_list:\n",
    "        rating = ratings.get(emotion, 0)\n",
    "        long_data.append({\n",
    "            'image': image,\n",
    "            'annotator': annotator,\n",
    "            'human': human,\n",
    "            'emotion': emotion,\n",
    "            'rating': rating\n",
    "        })\n",
    "df_long = pd.DataFrame(long_data)\n",
    "\n",
    "# Identify annotator groups\n",
    "all_annotators = df_long['annotator'].unique()\n",
    "human_annotators = df_long[df_long['human'] == True]['annotator'].unique()\n",
    "model_annotators = df_long[df_long['human'] == False]['annotator'].unique()\n",
    "\n",
    "# Pivot data for pairwise access per emotion\n",
    "df_pivot = df_long.pivot_table(index=['emotion', 'image'], columns='annotator', values='rating')\n",
    "\n",
    "# --- Pairwise Weighted Kappa Heatmap ---\n",
    "\n",
    "pairwise_results = []\n",
    "for emotion in emotion_list:\n",
    "    emotion_ratings = df_pivot.loc[emotion]\n",
    "    for r1, r2 in combinations(all_annotators, 2):\n",
    "        if r1 not in emotion_ratings.columns or r2 not in emotion_ratings.columns:\n",
    "            continue\n",
    "        ratings_pair = emotion_ratings[[r1, r2]].dropna()\n",
    "        if len(ratings_pair) < 2:\n",
    "            continue\n",
    "        ratings1 = ratings_pair[r1].astype(int)\n",
    "        ratings2 = ratings_pair[r2].astype(int)\n",
    "        try:\n",
    "            kappa_score = cohen_kappa_score(ratings1, ratings2, weights='quadratic', labels=list(range(8)))\n",
    "        except Exception:\n",
    "            kappa_score = np.nan\n",
    "        pairwise_results.append({\n",
    "            'emotion': emotion,\n",
    "            'rater1': r1,\n",
    "            'rater2': r2,\n",
    "            'weighted_kappa': kappa_score\n",
    "        })\n",
    "df_pairwise = pd.DataFrame(pairwise_results)\n",
    "\n",
    "# Average kappa per pair (across all emotions)\n",
    "avg_pairwise_kappa = df_pairwise.groupby(['rater1', 'rater2'])['weighted_kappa'].mean().reset_index()\n",
    "kappa_matrix_pivot = avg_pairwise_kappa.pivot(index='rater1', columns='rater2', values='weighted_kappa')\n",
    "\n",
    "# Custom annotator order: humans first\n",
    "sorted_humans = sorted(list(human_annotators))\n",
    "sorted_models = sorted(list(model_annotators))\n",
    "annotators_ordered = sorted_humans + sorted_models\n",
    "kappa_matrix = kappa_matrix_pivot.reindex(index=annotators_ordered, columns=annotators_ordered)\n",
    "\n",
    "# Fill symmetric values and diagonal\n",
    "for r1 in annotators_ordered:\n",
    "    for r2 in annotators_ordered:\n",
    "        if pd.isna(kappa_matrix.loc[r1, r2]) and r2 in kappa_matrix.index and r1 in kappa_matrix.columns and pd.notna(kappa_matrix.loc[r2, r1]):\n",
    "            kappa_matrix.loc[r1, r2] = kappa_matrix.loc[r2, r1]\n",
    "        if r1 == r2:\n",
    "            kappa_matrix.loc[r1, r2] = 1.0\n",
    "\n",
    "# Save heatmap as PNG and PDF\n",
    "fig_dir = Path(\"output\") / \"figures\"\n",
    "fig_dir.mkdir(parents=True, exist_ok=True)\n",
    "plt.figure(figsize=(max(8, len(annotators_ordered)*0.6), max(6, len(annotators_ordered)*0.5)))\n",
    "sns.heatmap(\n",
    "    kappa_matrix, annot=True, fmt=\".2f\", cmap=\"viridis\",\n",
    "    vmin=0, vmax=1, linewidths=.5, linecolor='lightgray',\n",
    "    cbar_kws={'label': 'Average Weighted Kappa (quadratic)'}\n",
    ")\n",
    "plt.title('Average Pairwise Weighted Kappa (Humans First)')\n",
    "plt.xlabel('Annotator 2')\n",
    "plt.ylabel('Annotator 1')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "fig_path_png = fig_dir / \"average_pairwise_kappa_heatmap_custom_order.png\"\n",
    "fig_path_pdf = fig_dir / \"average_pairwise_kappa_heatmap_custom_order.pdf\"\n",
    "plt.savefig(fig_path_png, dpi=300)\n",
    "plt.savefig(fig_path_pdf, dpi=300)\n",
    "plt.close()\n",
    "print(f\"Saved average pairwise Kappa heatmap to {fig_path_png} and {fig_path_pdf}\")\n",
    "\n",
    "# --- Model vs All Humans Agreement Linegraph ---\n",
    "\n",
    "model_vs_human_results = []\n",
    "for model in model_annotators:\n",
    "    for emotion in emotion_list:\n",
    "        emotion_ratings = df_pivot.loc[emotion]\n",
    "        kappas = []\n",
    "        for human in human_annotators:\n",
    "            if model not in emotion_ratings.columns or human not in emotion_ratings.columns:\n",
    "                continue\n",
    "            ratings_pair = emotion_ratings[[model, human]].dropna()\n",
    "            if len(ratings_pair) < 2:\n",
    "                continue\n",
    "            ratings1 = ratings_pair[model].astype(int)\n",
    "            ratings2 = ratings_pair[human].astype(int)\n",
    "            try:\n",
    "                kappa_score = cohen_kappa_score(ratings1, ratings2, weights='quadratic', labels=list(range(8)))\n",
    "            except Exception:\n",
    "                kappa_score = np.nan\n",
    "            kappas.append(kappa_score)\n",
    "        avg_kappa = np.nanmean(kappas) if kappas else np.nan\n",
    "        model_vs_human_results.append({\n",
    "            'model': model,\n",
    "            'emotion': emotion,\n",
    "            'avg_weighted_kappa': avg_kappa\n",
    "        })\n",
    "df_model_vs_human = pd.DataFrame(model_vs_human_results)\n",
    "\n",
    "# Plot linegraph: x=emotion, y=agreement, one line per model\n",
    "if not df_model_vs_human.empty:\n",
    "    plt.figure(figsize=(max(12, len(emotion_list) * 0.4), 7))\n",
    "    import itertools\n",
    "    markers = itertools.cycle(('o', 's', 'D', '^', 'v', '>', '<', 'p', '*', 'h', 'x'))\n",
    "    for model in model_annotators:\n",
    "        data = df_model_vs_human[df_model_vs_human['model'] == model]\n",
    "        plt.plot(\n",
    "            data['emotion'],\n",
    "            data['avg_weighted_kappa'],\n",
    "            marker=next(markers),\n",
    "            label=model\n",
    "        )\n",
    "    plt.title(\"Model vs All Humans: Agreement per Emotion\")\n",
    "    plt.xlabel(\"Emotion\")\n",
    "    plt.ylabel(\"Average Weighted Kappa (quadratic)\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.ylim(-0.1, 1.05)\n",
    "    plt.legend(title=\"Model\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
    "    line_path_png = fig_dir / \"model_vs_human_agreement_linegraph.png\"\n",
    "    line_path_pdf = fig_dir / \"model_vs_human_agreement_linegraph.pdf\"\n",
    "    plt.savefig(line_path_png, dpi=600)\n",
    "    plt.savefig(line_path_pdf, dpi=600)\n",
    "    plt.close()\n",
    "    print(f\"Saved model vs human agreement linegraph to {line_path_png} and {line_path_pdf}\")\n",
    "\n",
    "# --- Human vs All Other Humans Agreement Linegraph ---\n",
    "\n",
    "human_vs_human_results = []\n",
    "for human1 in human_annotators:\n",
    "    for emotion in emotion_list:\n",
    "        emotion_ratings = df_pivot.loc[emotion]\n",
    "        kappas = []\n",
    "        for human2 in human_annotators:\n",
    "            if human1 == human2:\n",
    "                continue\n",
    "            if human1 not in emotion_ratings.columns or human2 not in emotion_ratings.columns:\n",
    "                continue\n",
    "            ratings_pair = emotion_ratings[[human1, human2]].dropna()\n",
    "            if len(ratings_pair) < 2:\n",
    "                continue\n",
    "            ratings1 = ratings_pair[human1].astype(int)\n",
    "            ratings2 = ratings_pair[human2].astype(int)\n",
    "            try:\n",
    "                kappa_score = cohen_kappa_score(ratings1, ratings2, weights='quadratic', labels=list(range(8)))\n",
    "            except Exception:\n",
    "                kappa_score = np.nan\n",
    "            kappas.append(kappa_score)\n",
    "        avg_kappa = np.nanmean(kappas) if kappas else np.nan\n",
    "        human_vs_human_results.append({\n",
    "            'human': human1,\n",
    "            'emotion': emotion,\n",
    "            'avg_weighted_kappa': avg_kappa\n",
    "        })\n",
    "df_human_vs_human = pd.DataFrame(human_vs_human_results)\n",
    "\n",
    "# Plot linegraph: x=emotion, y=agreement, one line per human\n",
    "if not df_human_vs_human.empty:\n",
    "    plt.figure(figsize=(max(12, len(emotion_list) * 0.4), 7))\n",
    "    markers = itertools.cycle(('o', 's', 'D', '^', 'v', '>', '<', 'p', '*', 'h', 'x'))\n",
    "    for human in human_annotators:\n",
    "        data = df_human_vs_human[df_human_vs_human['human'] == human]\n",
    "        plt.plot(\n",
    "            data['emotion'],\n",
    "            data['avg_weighted_kappa'],\n",
    "            marker=next(markers),\n",
    "            label=human\n",
    "        )\n",
    "    plt.title(\"Human vs All Other Humans: Agreement per Emotion\")\n",
    "    plt.xlabel(\"Emotion\")\n",
    "    plt.ylabel(\"Average Weighted Kappa (quadratic)\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.ylim(-0.1, 1.05)\n",
    "    plt.legend(title=\"Human\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
    "    human_line_path_png = fig_dir / \"human_vs_human_agreement_linegraph.png\"\n",
    "    human_line_path_pdf = fig_dir / \"human_vs_human_agreement_linegraph.pdf\"\n",
    "    plt.savefig(human_line_path_png, dpi=600)\n",
    "    plt.savefig(human_line_path_pdf, dpi=600)\n",
    "    plt.close()\n",
    "    print(f\"Saved human vs human agreement linegraph to {human_line_path_png} and {human_line_path_pdf}\")\n",
    "\n",
    "# --- Agreement Table: Humans vs Humans, Models vs Humans ---\n",
    "\n",
    "# Compute agreement for each annotator with others (per emotion)\n",
    "human_emotion_agreement = {}\n",
    "for human1 in human_annotators:\n",
    "    row = {}\n",
    "    for emotion in emotion_list:\n",
    "        emotion_ratings = df_pivot.loc[emotion]\n",
    "        kappas = []\n",
    "        for human2 in human_annotators:\n",
    "            if human1 == human2:\n",
    "                continue\n",
    "            if human1 not in emotion_ratings.columns or human2 not in emotion_ratings.columns:\n",
    "                continue\n",
    "            ratings_pair = emotion_ratings[[human1, human2]].dropna()\n",
    "            if len(ratings_pair) < 2:\n",
    "                continue\n",
    "            ratings1 = ratings_pair[human1].astype(int)\n",
    "            ratings2 = ratings_pair[human2].astype(int)\n",
    "            try:\n",
    "                kappa_score = cohen_kappa_score(ratings1, ratings2, weights='quadratic', labels=list(range(8)))\n",
    "            except Exception:\n",
    "                kappa_score = np.nan\n",
    "            kappas.append(kappa_score)\n",
    "        row[emotion] = np.nanmean(kappas) if kappas else np.nan\n",
    "    human_emotion_agreement[human1] = row\n",
    "\n",
    "model_emotion_agreement = {}\n",
    "for model in model_annotators:\n",
    "    row = {}\n",
    "    for emotion in emotion_list:\n",
    "        emotion_ratings = df_pivot.loc[emotion]\n",
    "        kappas = []\n",
    "        for human in human_annotators:\n",
    "            if model not in emotion_ratings.columns or human not in emotion_ratings.columns:\n",
    "                continue\n",
    "            ratings_pair = emotion_ratings[[model, human]].dropna()\n",
    "            if len(ratings_pair) < 2:\n",
    "                continue\n",
    "            ratings1 = ratings_pair[model].astype(int)\n",
    "            ratings2 = ratings_pair[human].astype(int)\n",
    "            try:\n",
    "                kappa_score = cohen_kappa_score(ratings1, ratings2, weights='quadratic', labels=list(range(8)))\n",
    "            except Exception:\n",
    "                kappa_score = np.nan\n",
    "            kappas.append(kappa_score)\n",
    "        row[emotion] = np.nanmean(kappas) if kappas else np.nan\n",
    "    model_emotion_agreement[model] = row\n",
    "\n",
    "# Combine into one DataFrame and sort by average agreement\n",
    "agreement_table = pd.DataFrame.from_dict(\n",
    "    {**human_emotion_agreement, **model_emotion_agreement},\n",
    "    orient='index'\n",
    ")\n",
    "agreement_table['average_all_emotions'] = agreement_table[emotion_list].mean(axis=1)\n",
    "agreement_table.index.name = 'Annotator'\n",
    "agreement_table = agreement_table.reset_index()\n",
    "agreement_table = agreement_table.sort_values('average_all_emotions', ascending=False)\n",
    "\n",
    "# Save agreement table as CSV and LaTeX\n",
    "table_path = fig_dir / \"agreement_table_humans_and_models.csv\"\n",
    "agreement_table.to_csv(table_path, index=False)\n",
    "latex_path = fig_dir / \"agreement_table_humans_and_models.tex\"\n",
    "agreement_table.to_latex(latex_path, index=False, float_format=\"%.4f\")\n",
    "print(\"\\nAgreement Table (sorted by average_all_emotions):\")\n",
    "print(agreement_table.to_string(index=False))\n",
    "print(f\"Saved agreement table to {table_path} and {latex_path} (mean agreement per annotator and emotion).\")\n",
    "print(\"Agreement values are means across emotions; see output for details.\")\n",
    "\n",
    "# --- Summary Table: average + std + median + top-1 emotion ---\n",
    "\n",
    "summary_rows = []\n",
    "for _, row in agreement_table.iterrows():\n",
    "    annotator = row['Annotator']\n",
    "    values = [row[e] for e in emotion_list if pd.notnull(row[e])]\n",
    "    avg = np.mean(values) if values else np.nan\n",
    "    std = np.std(values, ddof=1) if len(values) > 1 else np.nan\n",
    "    median = np.median(values) if values else np.nan\n",
    "    # Get top-1 emotion by agreement for this annotator\n",
    "    emotion_scores = {e: row[e] for e in emotion_list}\n",
    "    sorted_emotions = sorted(emotion_scores.items(), key=lambda x: (-(x[1] if pd.notnull(x[1]) else -999)),)\n",
    "    tops = []\n",
    "    for i in range(5):\n",
    "        if i < len(sorted_emotions) and pd.notnull(sorted_emotions[i][1]):\n",
    "            tops.append(f\"{sorted_emotions[i][0]} ({sorted_emotions[i][1]:.4f})\")\n",
    "        else:\n",
    "            tops.append(\"\")\n",
    "    summary_rows.append({\n",
    "        \"Annotator\": annotator,\n",
    "        \"average_all_emotions\": avg,\n",
    "        \"std_all_emotions\": std,\n",
    "        \"median_all_emotions\": median,\n",
    "        \"top-1\": tops[0],\n",
    "        # \"top-2\": tops[1],\n",
    "        # \"top-3\": tops[2],\n",
    "        # \"top-4\": tops[3],\n",
    "        # \"top-5\": tops[4],\n",
    "    })\n",
    "summary_table = pd.DataFrame(summary_rows)\n",
    "summary_table = summary_table.sort_values(\"average_all_emotions\", ascending=False)\n",
    "\n",
    "# Save summary table as CSV and LaTeX (NeurIPS style)\n",
    "summary_csv_path = fig_dir / \"agreement_table_summary_humans_and_models.csv\"\n",
    "summary_tex_path = fig_dir / \"agreement_table_summary_humans_and_models.tex\"\n",
    "summary_table.to_csv(summary_csv_path, index=False)\n",
    "summary_table.to_latex(\n",
    "    summary_tex_path,\n",
    "    index=False,\n",
    "    float_format=\"%.4f\",\n",
    "    columns=[\n",
    "        \"Annotator\",\n",
    "        \"average_all_emotions\",\n",
    "        \"std_all_emotions\",\n",
    "        \"median_all_emotions\",\n",
    "        \"top-1\", \n",
    "        # \"top-2\", \"top-3\", \"top-4\", \"top-5\"\n",
    "    ],\n",
    "    caption=\"Agreement summary for all annotators: mean, standard deviation, median, and top-5 emotions (with highest agreement) per annotator. For humans, agreement is with all other humans; for models, agreement is with all humans.\",\n",
    "    label=\"tab:agreement_summary\"\n",
    ")\n",
    "\n",
    "print(\"\\nSummary Table (NeurIPS style, average/std/median/top-5):\")\n",
    "print(summary_table.to_string(index=False))\n",
    "print(f\"Saved summary table to {summary_csv_path} and {summary_tex_path} (mean, std, median, top-5 emotions per annotator).\")\n",
    "print(\"Error bars are standard deviation across emotions for each annotator.\")\n",
    "\n",
    "# --- Boxplot: Agreement by Annotator Group ---\n",
    "\n",
    "# Define annotator group mapping\n",
    "group_map = {}\n",
    "for annot in all_annotators:\n",
    "    if annot in human_annotators:\n",
    "        group_map[annot] = \"Human Annotators\"\n",
    "    elif \"Empathic Insight\" in annot:\n",
    "        group_map[annot] = \"Our Models\"\n",
    "    elif annot.endswith(\"MS\"):\n",
    "        group_map[annot] = \"VLMs with Multi-Shot Prompt\"\n",
    "    elif annot.endswith(\"ZS\"):\n",
    "        group_map[annot] = \"VLMs with Zero-Shot Prompt\"\n",
    "    elif \"Hume Face*\" in annot:\n",
    "        group_map[annot] = \"Proprietary Models\"\n",
    "    elif \"Random Baseline\" in annot:\n",
    "        group_map[annot] = \"Random Baseline\"\n",
    "    else:\n",
    "        print(f\"Warning: Unknown model type for '{annot}'\")\n",
    "        raise Exception(f\"Unknown model type: {annot}\")\n",
    "\n",
    "# Prepare data for boxplot: each row is (group, agreement)\n",
    "boxplot_data = []\n",
    "for _, row in agreement_table.iterrows():\n",
    "    annot = row[\"Annotator\"]\n",
    "    group = group_map.get(annot, \"other\")\n",
    "    for emotion in emotion_list:\n",
    "        val = row[emotion]\n",
    "        if pd.notnull(val):\n",
    "            boxplot_data.append({\"group\": group, \"agreement\": val})\n",
    "\n",
    "df_boxplot = pd.DataFrame(boxplot_data)\n",
    "\n",
    "# Plot boxplot (swapped axes: group on y, agreement on x)\n",
    "plt.figure(figsize=(10, 3.5))\n",
    "sns.boxplot(\n",
    "    data=df_boxplot, y=\"group\", x=\"agreement\",\n",
    "    order=[\n",
    "        \"Human Annotators\", \"Our Models\", \"Proprietary Models\",\n",
    "        \"VLMs with Multi-Shot Prompt\", \"VLMs with Zero-Shot Prompt\", \"Random Baseline\"\n",
    "    ]\n",
    ")\n",
    "plt.title(\"Agreement by Annotator Group\")\n",
    "plt.ylabel(\"Annotator Group\")\n",
    "plt.xlabel(\"Agreement (Weighted Kappa)\")\n",
    "plt.xlim(-0.1, 0.7)\n",
    "plt.tight_layout()\n",
    "boxplot_png = fig_dir / \"agreement_by_group_boxplot.png\"\n",
    "boxplot_pdf = fig_dir / \"agreement_by_group_boxplot.pdf\"\n",
    "plt.savefig(boxplot_png, dpi=600)\n",
    "plt.savefig(boxplot_pdf, dpi=600)\n",
    "plt.close()\n",
    "print(f\"Saved agreement by group boxplot to {boxplot_png} and {boxplot_pdf}\")\n",
    "print(\"Statistical significance between groups is assessed by Mann-Whitney U test;\")\n",
    "print(\"error bars are 95% bootstrap confidence intervals for mean differences.\")\n",
    "\n",
    "# --- Save pairwise group mean differences and 95% bootstrap CIs to CSV ---\n",
    "\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "group_order = [\n",
    "    \"Human Annotators\",\n",
    "    \"Our Models\",\n",
    "    \"Proprietary Models\",\n",
    "    \"VLMs with Multi-Shot Prompt\",\n",
    "    \"VLMs with Zero-Shot Prompt\",\n",
    "    \"Random Baseline\"\n",
    "]\n",
    "pairs = []\n",
    "for i in range(len(group_order)):\n",
    "    for j in range(i + 1, len(group_order)):\n",
    "        pairs.append((group_order[i], group_order[j]))\n",
    "\n",
    "pairwise_stats = []\n",
    "for g1, g2 in pairs:\n",
    "    data1 = df_boxplot[df_boxplot[\"group\"] == g1][\"agreement\"].dropna()\n",
    "    data2 = df_boxplot[df_boxplot[\"group\"] == g2][\"agreement\"].dropna()\n",
    "    if len(data1) > 1 and len(data2) > 1:\n",
    "        stat, pval = mannwhitneyu(data1, data2, alternative='two-sided')\n",
    "        diff = data1.mean() - data2.mean()\n",
    "        # Bootstrap CI for mean difference\n",
    "        boot_diffs = []\n",
    "        n_boot = 1000\n",
    "        rng = np.random.default_rng(42)\n",
    "        for _ in range(n_boot):\n",
    "            boot1 = rng.choice(data1, size=len(data1), replace=True)\n",
    "            boot2 = rng.choice(data2, size=len(data2), replace=True)\n",
    "            boot_diffs.append(np.mean(boot1) - np.mean(boot2))\n",
    "        ci_low, ci_high = np.percentile(boot_diffs, [2.5, 97.5])\n",
    "        pairwise_stats.append({\n",
    "            \"group1\": g1,\n",
    "            \"group2\": g2,\n",
    "            \"mean_difference\": diff,\n",
    "            \"ci_low\": ci_low,\n",
    "            \"ci_high\": ci_high,\n",
    "            \"p_value\": pval\n",
    "        })\n",
    "pairwise_stats_df = pd.DataFrame(pairwise_stats)\n",
    "pairwise_stats_csv = fig_dir / \"agreement_by_group_pairwise_stats.csv\"\n",
    "pairwise_stats_df.to_csv(pairwise_stats_csv, index=False)\n",
    "print(f\"Saved agreement by group pairwise stats to {pairwise_stats_csv}\")\n",
    "\n",
    "# --- Statistical comparison: Are models and humans on par with human annotators? ---\n",
    "\n",
    "model_human_stats = []\n",
    "\n",
    "def get_hh_kappas(exclude_human=None):\n",
    "    \"\"\"\n",
    "    Helper: get human-human kappas, excluding a specific human if provided.\n",
    "    \"\"\"\n",
    "    hh_kappas = []\n",
    "    for h1 in human_annotators:\n",
    "        if exclude_human is not None and h1 == exclude_human:\n",
    "            continue\n",
    "        for h2 in human_annotators:\n",
    "            if h1 >= h2 or (exclude_human is not None and h2 == exclude_human):\n",
    "                continue\n",
    "            for emotion in emotion_list:\n",
    "                emotion_ratings = df_pivot.loc[emotion]\n",
    "                if h1 not in emotion_ratings.columns or h2 not in emotion_ratings.columns:\n",
    "                    continue\n",
    "                ratings_pair = emotion_ratings[[h1, h2]].dropna()\n",
    "                if len(ratings_pair) < 2:\n",
    "                    continue\n",
    "                ratings1 = ratings_pair[h1].astype(int)\n",
    "                ratings2 = ratings_pair[h2].astype(int)\n",
    "                try:\n",
    "                    kappa = cohen_kappa_score(ratings1, ratings2, weights='quadratic', labels=list(range(8)))\n",
    "                except Exception:\n",
    "                    kappa = np.nan\n",
    "                if pd.notnull(kappa):\n",
    "                    hh_kappas.append(kappa)\n",
    "    return np.array(hh_kappas)[~np.isnan(hh_kappas)]\n",
    "\n",
    "# For each model, compare model-human distribution to human-human (all humans)\n",
    "hh_kappas_all = get_hh_kappas()\n",
    "for model in model_annotators:\n",
    "    mh_kappas = []\n",
    "    for human in human_annotators:\n",
    "        for emotion in emotion_list:\n",
    "            emotion_ratings = df_pivot.loc[emotion]\n",
    "            if model not in emotion_ratings.columns or human not in emotion_ratings.columns:\n",
    "                continue\n",
    "            ratings_pair = emotion_ratings[[model, human]].dropna()\n",
    "            if len(ratings_pair) < 2:\n",
    "                continue\n",
    "            ratings1 = ratings_pair[model].astype(int)\n",
    "            ratings2 = ratings_pair[human].astype(int)\n",
    "            try:\n",
    "                kappa = cohen_kappa_score(ratings1, ratings2, weights='quadratic', labels=list(range(8)))\n",
    "            except Exception:\n",
    "                kappa = np.nan\n",
    "            if pd.notnull(kappa):\n",
    "                mh_kappas.append(kappa)\n",
    "    mh_kappas = np.array(mh_kappas)\n",
    "    mh_kappas = mh_kappas[~np.isnan(mh_kappas)]\n",
    "\n",
    "    mean_diff = np.mean(mh_kappas) - np.mean(hh_kappas_all) if len(mh_kappas) > 0 and len(hh_kappas_all) > 0 else np.nan\n",
    "\n",
    "    n_boot = 1000\n",
    "    rng = np.random.default_rng(42)\n",
    "    boot_diffs = []\n",
    "    for _ in range(n_boot):\n",
    "        boot_mh = rng.choice(mh_kappas, size=len(mh_kappas), replace=True) if len(mh_kappas) > 0 else np.array([np.nan])\n",
    "        boot_hh = rng.choice(hh_kappas_all, size=len(hh_kappas_all), replace=True) if len(hh_kappas_all) > 0 else np.array([np.nan])\n",
    "        boot_diffs.append(np.nanmean(boot_mh) - np.nanmean(boot_hh))\n",
    "    ci_low, ci_high = np.percentile(boot_diffs, [2.5, 97.5])\n",
    "\n",
    "    if len(mh_kappas) > 1 and len(hh_kappas_all) > 1:\n",
    "        stat, pval = mannwhitneyu(mh_kappas, hh_kappas_all, alternative='two-sided')\n",
    "    else:\n",
    "        pval = np.nan\n",
    "\n",
    "    could_be_annotator = \"yes\" if (pval > 0.05 if not pd.isnull(pval) else False) else \"no\"\n",
    "\n",
    "    model_human_stats.append({\n",
    "        \"annotator\": model,\n",
    "        \"mean_difference\": mean_diff,\n",
    "        \"ci_of_difference\": f\"[{ci_low:.4f}, {ci_high:.4f}]\",\n",
    "        \"p_value\": pval,\n",
    "        \"could_be_annotator\": could_be_annotator\n",
    "    })\n",
    "\n",
    "# For each human, compare their human-vs-human distribution (excluding self) to the rest of humans\n",
    "for human in human_annotators:\n",
    "    hvh_kappas = []\n",
    "    for other in human_annotators:\n",
    "        if other == human:\n",
    "            continue\n",
    "        for emotion in emotion_list:\n",
    "            emotion_ratings = df_pivot.loc[emotion]\n",
    "            if human not in emotion_ratings.columns or other not in emotion_ratings.columns:\n",
    "                continue\n",
    "            ratings_pair = emotion_ratings[[human, other]].dropna()\n",
    "            if len(ratings_pair) < 2:\n",
    "                continue\n",
    "            ratings1 = ratings_pair[human].astype(int)\n",
    "            ratings2 = ratings_pair[other].astype(int)\n",
    "            try:\n",
    "                kappa = cohen_kappa_score(ratings1, ratings2, weights='quadratic', labels=list(range(8)))\n",
    "            except Exception:\n",
    "                kappa = np.nan\n",
    "            if pd.notnull(kappa):\n",
    "                hvh_kappas.append(kappa)\n",
    "    hvh_kappas = np.array(hvh_kappas)\n",
    "    hvh_kappas = hvh_kappas[~np.isnan(hvh_kappas)]\n",
    "\n",
    "    # human-human kappas for the rest (excluding this human)\n",
    "    hh_kappas_excl = get_hh_kappas(exclude_human=human)\n",
    "\n",
    "    mean_diff = np.mean(hvh_kappas) - np.mean(hh_kappas_excl) if len(hvh_kappas) > 0 and len(hh_kappas_excl) > 0 else np.nan\n",
    "\n",
    "    n_boot = 1000\n",
    "    rng = np.random.default_rng(42)\n",
    "    boot_diffs = []\n",
    "    for _ in range(n_boot):\n",
    "        boot_hvh = rng.choice(hvh_kappas, size=len(hvh_kappas), replace=True) if len(hvh_kappas) > 0 else np.array([np.nan])\n",
    "        boot_hh = rng.choice(hh_kappas_excl, size=len(hh_kappas_excl), replace=True) if len(hh_kappas_excl) > 0 else np.array([np.nan])\n",
    "        boot_diffs.append(np.nanmean(boot_hvh) - np.nanmean(boot_hh))\n",
    "    ci_low, ci_high = np.percentile(boot_diffs, [2.5, 97.5])\n",
    "\n",
    "    if len(hvh_kappas) > 1 and len(hh_kappas_excl) > 1:\n",
    "        stat, pval = mannwhitneyu(hvh_kappas, hh_kappas_excl, alternative='two-sided')\n",
    "    else:\n",
    "        pval = np.nan\n",
    "\n",
    "    could_be_annotator = \"yes\" if (pval > 0.05 if not pd.isnull(pval) else False) else \"no\"\n",
    "\n",
    "    model_human_stats.append({\n",
    "        \"annotator\": human,\n",
    "        \"mean_difference\": mean_diff,\n",
    "        \"ci_of_difference\": f\"[{ci_low:.4f}, {ci_high:.4f}]\",\n",
    "        \"p_value\": pval,\n",
    "        \"could_be_annotator\": could_be_annotator\n",
    "    })\n",
    "\n",
    "df_model_human_stats = pd.DataFrame(model_human_stats)\n",
    "\n",
    "# Sort by mean_difference descending\n",
    "df_model_human_stats = df_model_human_stats.sort_values(\"mean_difference\", ascending=False)\n",
    "stats_csv_path = fig_dir / \"model_vs_human_stats.csv\"\n",
    "stats_tex_path = fig_dir / \"model_vs_human_stats.tex\"\n",
    "df_model_human_stats.to_csv(stats_csv_path, index=False)\n",
    "df_model_human_stats.to_latex(\n",
    "    stats_tex_path,\n",
    "    index=False,\n",
    "    float_format=\"%.4f\",\n",
    "    columns=[\"annotator\", \"mean_difference\", \"ci_of_difference\", \"p_value\", \"could_be_annotator\"],\n",
    "    caption=\"Statistical comparison of each model and human's agreement with humans vs. the rest of human-human agreement. 'Could be annotator' is 'yes' if p > 0.05.\",\n",
    "    label=\"tab:model_vs_human_stats\"\n",
    ")\n",
    "print(\"\\nModel/Human vs Human Annotator Statistical Comparison Table:\")\n",
    "print(df_model_human_stats.to_string(index=False))\n",
    "print(f\"Saved model/human vs human statistical comparison to {stats_csv_path} and {stats_tex_path}.\")\n",
    "print(\"Mean differences and 95% bootstrap confidence intervals are reported;\")\n",
    "print(\"p-values from Mann-Whitney U test are included.\")\n",
    "\n",
    "# --- Krippendorff's Alpha: Human Inter-Annotator Reliability ---\n",
    "\n",
    "try:\n",
    "    import krippendorff\n",
    "except ImportError:\n",
    "    krippendorff = None\n",
    "    print(\"krippendorff package not installed. Skipping Krippendorff's alpha plot.\")\n",
    "\n",
    "if krippendorff is not None:\n",
    "    # Only use human annotators\n",
    "    df_humans = df_long[df_long['human'] == True]\n",
    "    alpha_per_emotion = []\n",
    "    alpha_ci_lows = []\n",
    "    alpha_ci_highs = []\n",
    "    alpha_boots_per_emotion = []\n",
    "    n_boot = 1000\n",
    "    rng = np.random.default_rng(42)\n",
    "    for emotion in emotion_list:\n",
    "        # Build matrix: rows=images, columns=annotators, values=rating\n",
    "        pivot = df_humans[df_humans['emotion'] == emotion].pivot_table(\n",
    "            index=\"image\", columns=\"annotator\", values=\"rating\"\n",
    "        )\n",
    "        # Build 2D numpy array (annotators x items), np.nan for missing\n",
    "        data = pivot.T.values\n",
    "        # Only keep annotators with at least 2 ratings\n",
    "        data = [row for row in data if np.count_nonzero(~np.isnan(row)) > 1]\n",
    "        if len(data) < 2:\n",
    "            alpha = None\n",
    "            ci_low = None\n",
    "            ci_high = None\n",
    "            boot_alphas = []\n",
    "        else:\n",
    "            data_arr = np.array(data)\n",
    "            alpha = krippendorff.alpha(reliability_data=data_arr, level_of_measurement='interval')\n",
    "            # Bootstrap CI over images (columns)\n",
    "            boot_alphas = []\n",
    "            n_items = data_arr.shape[1]\n",
    "            for _ in range(n_boot):\n",
    "                # Sample images (columns) with replacement\n",
    "                idxs = rng.choice(n_items, size=n_items, replace=True)\n",
    "                boot_data = data_arr[:, idxs]\n",
    "                try:\n",
    "                    boot_alpha = krippendorff.alpha(reliability_data=boot_data, level_of_measurement='interval')\n",
    "                except Exception:\n",
    "                    boot_alpha = np.nan\n",
    "                boot_alphas.append(boot_alpha)\n",
    "            boot_alphas = np.array([a for a in boot_alphas if not pd.isnull(a)])\n",
    "            if len(boot_alphas) > 0:\n",
    "                ci_low, ci_high = np.percentile(boot_alphas, [2.5, 97.5])\n",
    "            else:\n",
    "                ci_low, ci_high = np.nan, np.nan\n",
    "        alpha_per_emotion.append(alpha)\n",
    "        alpha_ci_lows.append(ci_low)\n",
    "        alpha_ci_highs.append(ci_high)\n",
    "        alpha_boots_per_emotion.append(boot_alphas)\n",
    "    # Shorten emotion labels: keep only part after \"|\"\n",
    "    def short_label(e):\n",
    "        return e.split(\"|\")[-1].strip()\n",
    "    short_emotion_labels = [short_label(emotion_list[i]) for i in range(len(emotion_list))]\n",
    "    # Sort by alpha descending\n",
    "    sorted_indices = np.argsort(alpha_per_emotion)[::-1]\n",
    "    sorted_emotions = [short_emotion_labels[i] for i in sorted_indices]\n",
    "    sorted_boots = [alpha_boots_per_emotion[i] for i in sorted_indices]\n",
    "    sorted_alphas = [alpha_per_emotion[i] for i in sorted_indices]\n",
    "    sorted_ci_lows = [alpha_ci_lows[i] for i in sorted_indices]\n",
    "    sorted_ci_highs = [alpha_ci_highs[i] for i in sorted_indices]\n",
    "\n",
    "    # Boxplot for Krippendorff's alpha bootstrap distribution per emotion\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    plt.figure(figsize=(10, max(8, len(emotion_list)*0.35)))\n",
    "    box_data = sorted_boots\n",
    "    plt.boxplot(\n",
    "        box_data,\n",
    "        vert=False,\n",
    "        patch_artist=True,\n",
    "        labels=sorted_emotions,\n",
    "        showmeans=False,\n",
    "        boxprops=dict(facecolor='lightblue', color='C0'),\n",
    "        medianprops=dict(color='C0', linewidth=2),\n",
    "        whiskerprops=dict(color='C0'),\n",
    "        capprops=dict(color='C0'),\n",
    "        flierprops=dict(markerfacecolor='gray', marker='o', markersize=3, alpha=0.3)\n",
    "    )\n",
    "    # Save alpha summary to CSV\n",
    "    alpha_box_csv = fig_dir / \"krippendorff_alpha_per_emotion_boxplot.csv\"\n",
    "    alpha_box_df = pd.DataFrame({\n",
    "        \"emotion\": sorted_emotions,\n",
    "        \"alpha\": sorted_alphas,\n",
    "        \"ci_low\": sorted_ci_lows,\n",
    "        \"ci_high\": sorted_ci_highs\n",
    "    })\n",
    "    # Add mean and median rows\n",
    "    mean_row = pd.DataFrame([{\n",
    "        \"emotion\": \"mean\",\n",
    "        \"alpha\": np.nanmean(sorted_alphas),\n",
    "        \"ci_low\": np.nanmean(sorted_ci_lows),\n",
    "        \"ci_high\": np.nanmean(sorted_ci_highs)\n",
    "    }])\n",
    "    median_row = pd.DataFrame([{\n",
    "        \"emotion\": \"median\",\n",
    "        \"alpha\": np.nanmedian(sorted_alphas),\n",
    "        \"ci_low\": np.nanmedian(sorted_ci_lows),\n",
    "        \"ci_high\": np.nanmedian(sorted_ci_highs)\n",
    "    }])\n",
    "    alpha_box_df = pd.concat([alpha_box_df, mean_row, median_row], ignore_index=True)\n",
    "    alpha_box_df = alpha_box_df.sort_values(\"alpha\", ascending=False)\n",
    "    alpha_box_df.to_csv(alpha_box_csv, index=False)\n",
    "    print(f\"Saved Krippendorff's alpha boxplot data to {alpha_box_csv}\")\n",
    "\n",
    "    plt.xlabel(\"Krippendorff's α (interval)\")\n",
    "    plt.ylabel(\"Emotion\")\n",
    "    plt.title(\"Human Inter-Annotator Reliability (Krippendorff's α)\")\n",
    "    plt.tight_layout()\n",
    "    alpha_box_png = fig_dir / \"_krippendorff_alpha_per_emotion_boxplot.png\"\n",
    "    alpha_box_pdf = fig_dir / \"_krippendorff_alpha_per_emotion_boxplot.pdf\"\n",
    "    plt.savefig(alpha_box_png, dpi=1200)\n",
    "    plt.savefig(alpha_box_pdf, dpi=1200)\n",
    "    plt.close()\n",
    "    print(f\"Saved Krippendorff's alpha boxplot to {alpha_box_png} and {alpha_box_pdf}\")\n",
    "\n",
    "# --- Model vs Human Consensus: Spearman Only, Sorted Barplots with Error Bars ---\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "metrics = [\"spearman_rho\"]\n",
    "agg_types = [\"median\"]\n",
    "model_human_metrics = {agg: [] for agg in agg_types}\n",
    "\n",
    "# Define annotator group mapping for bar textures\n",
    "group_map = {}\n",
    "for annot in all_annotators:\n",
    "    if annot in human_annotators:\n",
    "        group_map[annot] = \"Human Annotators\"\n",
    "    elif \"Empathic Insight\" in annot:\n",
    "        group_map[annot] = \"Our Models\"\n",
    "    elif annot.endswith(\"MS\"):\n",
    "        group_map[annot] = \"VLMs with Multi-Shot Prompt\"\n",
    "    elif annot.endswith(\"ZS\"):\n",
    "        group_map[annot] = \"VLMs with Zero-Shot Prompt\"\n",
    "    elif \"Hume Face*\" in annot:\n",
    "        group_map[annot] = \"Proprietary Models\"\n",
    "    elif \"Random Baseline\" in annot:\n",
    "        group_map[annot] = \"Random Baseline\"\n",
    "    else:\n",
    "        print(f\"Warning: Unknown model type for '{annot}'\")\n",
    "        raise Exception(f\"Unknown model type: {annot}\")\n",
    "\n",
    "# Define hatches for each group\n",
    "hatch_map = {\n",
    "    \"Human Annotators\": \"\",\n",
    "    \"Our Models\": \"**\",\n",
    "    \"VLMs with Multi-Shot Prompt\": \"x\",\n",
    "    \"VLMs with Zero-Shot Prompt\": \"..\",\n",
    "    \"Random Baseline\": \"++\",\n",
    "    \"Proprietary Models\": \"//\"\n",
    "}\n",
    "\n",
    "for agg in agg_types:\n",
    "    for model in model_annotators:\n",
    "        for emotion in emotion_list:\n",
    "            emotion_ratings = df_pivot.loc[emotion]\n",
    "            if model not in emotion_ratings.columns:\n",
    "                row = {\"model\": model, \"emotion\": emotion}\n",
    "                for m in metrics:\n",
    "                    row[m] = None\n",
    "                model_human_metrics[agg].append(row)\n",
    "                continue\n",
    "            model_ratings = emotion_ratings[model]\n",
    "            human_cols = [h for h in human_annotators if h in emotion_ratings.columns]\n",
    "            if not human_cols:\n",
    "                row = {\"model\": model, \"emotion\": emotion}\n",
    "                for m in metrics:\n",
    "                    row[m] = None\n",
    "                model_human_metrics[agg].append(row)\n",
    "                continue\n",
    "            if agg == \"mean\":\n",
    "                human_agg = emotion_ratings[human_cols].mean(axis=1)\n",
    "            else:\n",
    "                human_agg = emotion_ratings[human_cols].median(axis=1)\n",
    "            valid_idx = model_ratings.dropna().index.intersection(human_agg.dropna().index)\n",
    "            if len(valid_idx) < 2:\n",
    "                row = {\"model\": model, \"emotion\": emotion}\n",
    "                for m in metrics:\n",
    "                    row[m] = None\n",
    "                model_human_metrics[agg].append(row)\n",
    "                continue\n",
    "            mvals = model_ratings.loc[valid_idx]\n",
    "            hvals = human_agg.loc[valid_idx]\n",
    "            # Spearman correlation\n",
    "            try:\n",
    "                rho, _ = spearmanr(mvals, hvals)\n",
    "            except Exception:\n",
    "                rho = None\n",
    "            model_human_metrics[agg].append({\n",
    "                \"model\": model,\n",
    "                \"emotion\": emotion,\n",
    "                \"spearman_rho\": rho\n",
    "            })\n",
    "\n",
    "# Aggregate across all emotions for each model and plot sorted barplots with error bars and textures\n",
    "n_boot = 1000\n",
    "rng = np.random.default_rng(42)\n",
    "for agg in agg_types:\n",
    "    df_metrics = pd.DataFrame(model_human_metrics[agg])\n",
    "    for metric in metrics:\n",
    "        # Aggregate: mean and bootstrap CI across all emotions for each model\n",
    "        agg_df = df_metrics.groupby(\"model\")[metric].agg(['mean']).reset_index()\n",
    "        # Bootstrap 95% CI for mean\n",
    "        ci_lows = []\n",
    "        ci_highs = []\n",
    "        for model in agg_df[\"model\"]:\n",
    "            vals = df_metrics[df_metrics[\"model\"] == model][metric].dropna().values\n",
    "            if len(vals) < 2:\n",
    "                ci_lows.append(np.nan)\n",
    "                ci_highs.append(np.nan)\n",
    "                continue\n",
    "            boots = [rng.choice(vals, size=len(vals), replace=True).mean() for _ in range(n_boot)]\n",
    "            ci_low, ci_high = np.percentile(boots, [2.5, 97.5])\n",
    "            ci_lows.append(ci_low)\n",
    "            ci_highs.append(ci_high)\n",
    "        agg_df[\"ci_low\"] = ci_lows\n",
    "        agg_df[\"ci_high\"] = ci_highs\n",
    "        # Sort by mean descending\n",
    "        agg_df = agg_df.sort_values(\"mean\", ascending=False)\n",
    "        # Get group and hatch for each model\n",
    "        agg_df[\"group\"] = agg_df[\"model\"].map(group_map).fillna(\"other\")\n",
    "        agg_df[\"hatch\"] = agg_df[\"group\"].map(hatch_map).fillna(\"\")\n",
    "        # Plot with error bars and textures\n",
    "        plt.figure(figsize=(max(8, len(model_annotators)*0.5), 5))\n",
    "\n",
    "        bars = plt.bar(\n",
    "            agg_df[\"model\"], agg_df[\"mean\"], color='#7ec8e3',\n",
    "            yerr=[agg_df[\"mean\"]-agg_df[\"ci_low\"], agg_df[\"ci_high\"]-agg_df[\"mean\"]],\n",
    "            capsize=6\n",
    "        )\n",
    "        # Apply hatches\n",
    "        for bar, hatch in zip(bars, agg_df[\"hatch\"]):\n",
    "            bar.set_hatch(hatch)\n",
    "        # Legend for textures\n",
    "        legend_handles = []\n",
    "        seen = set()\n",
    "        import matplotlib.patches as mpatches\n",
    "        for group, hatch in hatch_map.items():\n",
    "            if group in agg_df[\"group\"].values and group not in seen:\n",
    "                patch = mpatches.Patch(facecolor='#7ec8e3', hatch=hatch, label=group)\n",
    "                legend_handles.append(patch)\n",
    "                seen.add(group)\n",
    "        plt.legend(handles=legend_handles, title=\"Annotator Group\", loc=\"upper right\")\n",
    "        plt.ylabel(metric.replace(\"_\", \" \").title())\n",
    "        plt.xlabel(\"Model Annotator\")\n",
    "        plt.title(f\"Model vs Human ({agg.title()} Aggregation): Mean {metric.replace('_', ' ').title()} Across All Emotions\")\n",
    "        plt.ylim(-0.1, 0.75)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        fname = f\"_model_vs_human_{agg}_{metric}_aggregate_barplot\"\n",
    "        plt.savefig(fig_dir / f\"{fname}.png\", dpi=300)\n",
    "        plt.savefig(fig_dir / f\"{fname}.pdf\", dpi=300)\n",
    "        plt.close()\n",
    "        print(f\"Saved aggregate {metric} barplot ({agg}) to {fig_dir / (fname + '.png')} and .pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
